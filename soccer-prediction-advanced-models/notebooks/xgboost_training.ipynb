{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Training\n",
    "This notebook implements the training process for the XGBoost model to predict target variables in the soccer dataset. It includes data preparation, model training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.8.0)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/lionlucky7/01.Projects/In-progress/soccer_prediction/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set visualization style\n",
    "sns.set(style='whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../data/final/final.csv')\n",
    "\n",
    "# Data preprocessing (assuming preprocessing functions are defined in src/data/preprocessing.py)\n",
    "from src.data.preprocessing import preprocess_data\n",
    "df_clean = preprocess_data(df)\n",
    "\n",
    "# Define features and targets\n",
    "FEATURES = [\n",
    "    'Playing_Time_Min', 'Playing_Time_90s', 'Starts_Starts',\n",
    "    'Per_90_Minutes_Gls', 'Per_90_Minutes_Ast', 'Per_90_Minutes_xG',\n",
    "    'Standard_SoT%', 'KP', 'Ast', 'Tkl+Int', 'Blocks_Blocks'\n",
    "]\n",
    "\n",
    "TARGETS = [\n",
    "    'Per_90_Minutes_npxG', 'Per_90_Minutes_xG', 'Standard_Sh/90'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X = df_clean[FEATURES]\n",
    "y = df_clean[TARGETS]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f'Training samples: {X_train.shape[0]}, Testing samples: {X_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the XGBoost model\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_train = xgb_model.predict(X_train)\n",
    "y_pred_test = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "results = {\n",
    "    'Train MAE': mean_absolute_error(y_train, y_pred_train),\n",
    "    'Test MAE': mean_absolute_error(y_test, y_pred_test),\n",
    "    'Train R²': r2_score(y_train, y_pred_train),\n",
    "    'Test R²': r2_score(y_test, y_pred_test)\n",
    "}\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(X.columns, xgb_model.feature_importances_, color='skyblue')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize loss curves\n",
    "train_errors, test_errors = [], []\n",
    "for m in range(1, 101):\n",
    "    xgb_model = XGBRegressor(n_estimators=m, random_state=42)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    y_train_predict = xgb_model.predict(X_train)\n",
    "    y_test_predict = xgb_model.predict(X_test)\n",
    "    train_errors.append(mean_squared_error(y_train, y_train_predict))\n",
    "    test_errors.append(mean_squared_error(y_test, y_test_predict))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, 101), train_errors, label='Train MSE', color='blue')\n",
    "plt.plot(range(1, 101), test_errors, label='Test MSE', color='red')\n",
    "plt.title('Loss Curves for XGBoost')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".ipynb",
   "mimetype": "application/x-ipynb+json",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
