{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6a05a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import sys\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62816f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = Options()\n",
    "\n",
    "chrome_options.add_experimental_option(\"detach\", True)\n",
    "chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-logging\", \"enable-automation\"])\n",
    "\n",
    "# Add user agent to avoid bot detection\n",
    "chrome_options.add_argument('user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "\n",
    "# Performance optimizations\n",
    "chrome_options.add_argument('--disable-gpu')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "chrome_options.add_argument('--disable-extensions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b581008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# League IDs organized by tier\n",
    "\n",
    "IDs = {\n",
    "    # ============ TIER 1: Top European Leagues ============\n",
    "    \"Premier_League\": 13,      # England\n",
    "    \"La_Liga\": 53,             # Spain\n",
    "    \"Serie_A\": 31,             # Italy\n",
    "    \"Bundesliga\": 19,          # Germany\n",
    "    \"Ligue_1\": 16,             # France\n",
    "    \n",
    "    # ============ TIER 2: Second Divisions & Competitive Leagues ============\n",
    "    \"Championship\": 14,        # England (2nd tier)\n",
    "    \"La_Liga_2\": 54,           # Spain (2nd tier)\n",
    "    \"Eredivisie\": 10,          # Netherlands\n",
    "    \"Pro_League\": 4,           # Belgium\n",
    "    \n",
    "    # ============ TIER 3: Other Leagues ============\n",
    "    \"MLS\": 39,                 # United States\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2668a422",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEAGUES = \"\"\"\n",
    "lg%5B%5D=13&lg%5B%5D=31&lg%5B%5D=53&lg%5B%5D=19&lg%5B%5D=14&lg%5B%5D=54&lg%5B%5D=10&lg%5B%5D=4&lg%5B%5D=83&lg%5B%5D=2012\n",
    "\"\"\"\n",
    "\n",
    "LEAGUES = LEAGUES.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d55a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = f\"\"\"\n",
    "https://sofifa.com/?&showCol%5B%5D=pi&{LEAGUES}&showCol%5B%5D=oa&showCol%5B%5D=pt&showCol%5B%5D=tt&showCol%5B%5D=pi&showCol%5B%5D=by&showCol%5B%5D=hi&showCol%5B%5D=wi&showCol%5B%5D=pf&showCol%5B%5D=bo&showCol%5B%5D=bp&showCol%5B%5D=gu&showCol%5B%5D=jt&showCol%5B%5D=le&showCol%5B%5D=ta&showCol%5B%5D=cr&showCol%5B%5D=he&showCol%5B%5D=sh&showCol%5B%5D=vo&showCol%5B%5D=fi&showCol%5B%5D=ts&showCol%5B%5D=cu&showCol%5B%5D=dr&showCol%5B%5D=lo&showCol%5B%5D=bl&showCol%5B%5D=fr&showCol%5B%5D=ac&showCol%5B%5D=ag&showCol%5B%5D=re&showCol%5B%5D=sp&showCol%5B%5D=ba&showCol%5B%5D=to&showCol%5B%5D=tp&showCol%5B%5D=so&showCol%5B%5D=st&showCol%5B%5D=sr&showCol%5B%5D=ln&showCol%5B%5D=ju&showCol%5B%5D=ar&showCol%5B%5D=vi&showCol%5B%5D=po&showCol%5B%5D=pe&showCol%5B%5D=cm&showCol%5B%5D=in&showCol%5B%5D=te&showCol%5B%5D=sa&showCol%5B%5D=sl&showCol%5B%5D=ma&showCol%5B%5D=td&showCol%5B%5D=tg&showCol%5B%5D=gc&showCol%5B%5D=gh&showCol%5B%5D=gd&showCol%5B%5D=gr&showCol%5B%5D=gp&showCol%5B%5D=bs&showCol%5B%5D=wk&showCol%5B%5D=aw&showCol%5B%5D=ir&showCol%5B%5D=bt&showCol%5B%5D=dw&showCol%5B%5D=pac&showCol%5B%5D=sk&showCol%5B%5D=sho&showCol%5B%5D=pas&showCol%5B%5D=dri&showCol%5B%5D=def&showCol%5B%5D=phy&showCol%5B%5D=t1&showCol%5B%5D=ps1&showCol%5B%5D=ps2&showCol%5B%5D=tc&showCol%5B%5D=hc&showCol%5B%5D=t2&showCol%5B%5D=cp&showCol%5B%5D=at&showCol%5B%5D=wg&showCol%5B%5D=vl&showCol%5B%5D=rc&showCol%5B%5D=cj\"\"\"\n",
    "\n",
    "BASE_URL = BASE_URL.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68565919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIFA roster dates and their corresponding IDs (filtered to ~1 per month)\n",
    "roster_dates = {\n",
    "    # \"Sep 11, 2023\": 230054,\n",
    "    \"Aug 11, 2023\": 230050,\n",
    "    \"Jul 17, 2023\": 230045,\n",
    "    \"Jun 19, 2023\": 230040,\n",
    "    \"May 16, 2023\": 230034,\n",
    "    \"Apr 17, 2023\": 230028,\n",
    "    \"Mar 24, 2023\": 230021,\n",
    "    \"Feb 22, 2023\": 230015,\n",
    "    \"Jan 18, 2023\": 230010,\n",
    "    \"Dec 17, 2022\": 230008,\n",
    "    \"Nov 16, 2022\": 230006,\n",
    "    \"Oct 7, 2022\": 230004,\n",
    "    \"Sep 1, 2022\": 230001\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d0f20fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoFIFAScraper:\n",
    "    def __init__(self, seasons, base_url, save_dir):\n",
    "        self.seasons = seasons  # Dictionary: {date_string: season_code}\n",
    "        self.base_url = base_url\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "    def initialize_driver(self):        \n",
    "        chrome_options = Options()\n",
    "\n",
    "        chrome_options.add_experimental_option(\"detach\", True)\n",
    "        chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-logging\", \"enable-automation\"])\n",
    "\n",
    "        # Add user agent to avoid bot detection\n",
    "        chrome_options.add_argument('user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')\n",
    "\n",
    "        # Performance optimizations\n",
    "        chrome_options.add_argument('--disable-gpu')\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "        chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        chrome_options.add_argument('--disable-extensions')\n",
    "        \n",
    "        chrome_options.page_load_strategy = \"eager\"\n",
    "\n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        print(\"Chrome WebDriver initialized\")\n",
    "        \n",
    "        return driver\n",
    "\n",
    "\n",
    "    def get_player_stats(self, driver, season_code, roster_date):\n",
    "        \"\"\"\n",
    "        Scrape player stats for a given FIFA roster date.\n",
    "        \n",
    "        Args:\n",
    "            driver: Selenium WebDriver instance\n",
    "            season_code: SoFIFA season code (e.g., 230054)\n",
    "            roster_date: Roster date string (e.g., \"Sep 11, 2023\")\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with all players for the roster date\n",
    "        \"\"\"\n",
    "        data_list = []\n",
    "        offset = 0\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                url = f\"{self.base_url}&r={season_code}&set=true&offset={offset}\"\n",
    "                driver.get(url)\n",
    "                time.sleep(3)\n",
    "                \n",
    "                WebDriverWait(driver, 20).until(\n",
    "                    EC.presence_of_element_located((By.TAG_NAME, \"table\"))\n",
    "                )\n",
    "                \n",
    "            except TimeoutException:\n",
    "                print(f\"  ✓ Scraping complete at offset {offset}\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Error at offset {offset}: {e}\")\n",
    "                break\n",
    "            \n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            table = soup.find('table')\n",
    "            \n",
    "            if not table:\n",
    "                print(\"  ✗ ERROR: No table found!\")\n",
    "                break\n",
    "            \n",
    "            # Get headers only once\n",
    "            if offset == 0:\n",
    "                headers = [th.get_text(strip=True) for th in table.select(\"thead th\")]\n",
    "            \n",
    "            # Extract rows\n",
    "            for row in table.select(\"tbody tr\"):\n",
    "                cols = row.find_all(['th', 'td'])\n",
    "                if cols:\n",
    "                    data_list.append([col.get_text(strip=True) for col in cols])\n",
    "\n",
    "            offset += 60\n",
    "            \n",
    "            if offset % 600 == 0:\n",
    "                print(f\"  → {offset} players scraped...\")\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(data_list, columns=headers)\n",
    "        \n",
    "        # Add metadata\n",
    "        df['roster_date'] = roster_date\n",
    "        df['season_code'] = season_code\n",
    "\n",
    "        print(f\"  ✓ Total players collected: {len(df)}\")\n",
    "        return df\n",
    "\n",
    "    def close_driver(self, driver):\n",
    "        \"\"\"Close the WebDriver.\"\"\"\n",
    "        driver.quit()\n",
    "        print(\"Chrome WebDriver closed\")\n",
    "    \n",
    "    def scrape_all_separate(self):\n",
    "        \"\"\"Scrape all roster dates and save each to a separate CSV file.\"\"\"\n",
    "        # Ensure directory exists\n",
    "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        driver = self.initialize_driver()\n",
    "        \n",
    "        total_rows = 0\n",
    "        completed = 0\n",
    "        saved_files = []\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Starting SoFIFA Player Data Collection\")\n",
    "        print(f\"Total roster dates to scrape: {len(self.seasons)}\")\n",
    "        print(f\"Save location: {self.save_dir}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        try:\n",
    "            for roster_date, season_code in self.seasons.items():\n",
    "                completed += 1\n",
    "                print(f\"\\n[{completed}/{len(self.seasons)}] Scraping: {roster_date} (code: {season_code})\")\n",
    "                print(f\"{'-'*60}\")\n",
    "                \n",
    "                # Clear cookies between seasons\n",
    "                driver.delete_all_cookies()\n",
    "                \n",
    "                df = self.get_player_stats(driver, season_code, roster_date)\n",
    "                \n",
    "                # Create filename from season code\n",
    "                output_path = self.save_dir / f\"players_{season_code}.csv\"\n",
    "                \n",
    "                # Write to separate CSV file\n",
    "                df.to_csv(output_path, index=False)\n",
    "                \n",
    "                total_rows += len(df)\n",
    "                saved_files.append(output_path.name)\n",
    "                print(f\"  ✓ Saved {len(df)} rows to: {output_path.name}\")\n",
    "                \n",
    "                # Clear memory\n",
    "                del df\n",
    "        \n",
    "        finally:\n",
    "            self.close_driver(driver)\n",
    "            \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"✓ SCRAPING COMPLETE\")\n",
    "        print(f\"  • Total roster dates: {len(self.seasons)}\")\n",
    "        print(f\"  • Total players collected: {total_rows}\")\n",
    "        print(f\"  • Total files created: {len(saved_files)}\")\n",
    "        print(f\"  • Saved to: {self.save_dir}\")\n",
    "        print(f\"{'='*60}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01f7a334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/lionlucky7/01.Projects/In-progress/soccer_prediction/2026_world_cup\n",
      "Save directory: /Users/lionlucky7/01.Projects/In-progress/soccer_prediction/2026_world_cup/data/player_data_train\n",
      "Number of seasons to scrape: 12\n"
     ]
    }
   ],
   "source": [
    "# Set up paths and parameters\n",
    "project_root = Path(__file__).resolve().parents[2] if '__file__' in globals() else Path.cwd().parents[1]\n",
    "save_directory = project_root / \"data\" / \"player_data_train\"\n",
    "\n",
    "# Create scraper instance\n",
    "scraper = SoFIFAScraper(\n",
    "    seasons=roster_dates,\n",
    "    base_url=BASE_URL,\n",
    "    save_dir=save_directory\n",
    ")\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Save directory: {save_directory}\")\n",
    "print(f\"Number of seasons to scrape: {len(roster_dates)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df6a7bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chrome WebDriver initialized\n",
      "\n",
      "============================================================\n",
      "Starting SoFIFA Player Data Collection\n",
      "Total roster dates to scrape: 12\n",
      "Save location: /Users/lionlucky7/01.Projects/In-progress/soccer_prediction/2026_world_cup/data/player_data_train\n",
      "============================================================\n",
      "\n",
      "\n",
      "[1/12] Scraping: Aug 11, 2023 (code: 230050)\n",
      "------------------------------------------------------------\n",
      "  → 600 players scraped...\n",
      "  → 1200 players scraped...\n",
      "  → 1800 players scraped...\n",
      "  → 2400 players scraped...\n",
      "  → 3000 players scraped...\n",
      "  → 3600 players scraped...\n",
      "  → 4200 players scraped...\n",
      "  → 4800 players scraped...\n",
      "  → 5400 players scraped...\n",
      "  ✓ Scraping complete at offset 5580\n",
      "  ✓ Total players collected: 5539\n",
      "  ✓ Saved 5539 rows to: players_230050.csv\n",
      "\n",
      "[2/12] Scraping: Jul 17, 2023 (code: 230045)\n",
      "------------------------------------------------------------\n",
      "  → 600 players scraped...\n",
      "  → 1200 players scraped...\n",
      "  → 1800 players scraped...\n",
      "  → 2400 players scraped...\n",
      "  → 3000 players scraped...\n",
      "  → 3600 players scraped...\n",
      "  → 4200 players scraped...\n",
      "  → 4800 players scraped...\n",
      "  → 5400 players scraped...\n",
      "  ✓ Scraping complete at offset 5580\n",
      "  ✓ Total players collected: 5543\n",
      "  ✓ Saved 5543 rows to: players_230045.csv\n",
      "\n",
      "[3/12] Scraping: Jun 19, 2023 (code: 230040)\n",
      "------------------------------------------------------------\n",
      "  → 600 players scraped...\n",
      "  → 1200 players scraped...\n",
      "  → 1800 players scraped...\n",
      "  → 2400 players scraped...\n",
      "  → 3000 players scraped...\n",
      "  → 3600 players scraped...\n",
      "  → 4200 players scraped...\n",
      "  → 4800 players scraped...\n",
      "  → 5400 players scraped...\n",
      "  ✓ Scraping complete at offset 5580\n",
      "  ✓ Total players collected: 5556\n",
      "  ✓ Saved 5556 rows to: players_230040.csv\n",
      "\n",
      "[4/12] Scraping: May 16, 2023 (code: 230034)\n",
      "------------------------------------------------------------\n",
      "  → 600 players scraped...\n",
      "  → 1200 players scraped...\n",
      "  → 1800 players scraped...\n",
      "  → 2400 players scraped...\n",
      "  → 3000 players scraped...\n",
      "  → 3600 players scraped...\n",
      "  → 4200 players scraped...\n",
      "  → 4800 players scraped...\n",
      "  → 5400 players scraped...\n",
      "  ✓ Scraping complete at offset 5580\n",
      "  ✓ Total players collected: 5551\n",
      "  ✓ Saved 5551 rows to: players_230034.csv\n",
      "\n",
      "[5/12] Scraping: Apr 17, 2023 (code: 230028)\n",
      "------------------------------------------------------------\n",
      "  → 600 players scraped...\n",
      "  → 1200 players scraped...\n",
      "  → 1800 players scraped...\n",
      "  → 2400 players scraped...\n",
      "  → 3000 players scraped...\n",
      "  → 3600 players scraped...\n",
      "  → 4200 players scraped...\n",
      "  → 4800 players scraped...\n",
      "  → 5400 players scraped...\n",
      "  ✓ Scraping complete at offset 5580\n",
      "  ✓ Total players collected: 5546\n",
      "  ✓ Saved 5546 rows to: players_230028.csv\n",
      "\n",
      "[6/12] Scraping: Mar 24, 2023 (code: 230021)\n",
      "------------------------------------------------------------\n",
      "  → 600 players scraped...\n",
      "  → 1200 players scraped...\n",
      "  → 1800 players scraped...\n",
      "  → 2400 players scraped...\n",
      "  → 3000 players scraped...\n",
      "  → 3600 players scraped...\n",
      "  → 4200 players scraped...\n",
      "  → 4800 players scraped...\n",
      "  → 5400 players scraped...\n",
      "  ✓ Scraping complete at offset 5580\n",
      "  ✓ Total players collected: 5542\n",
      "  ✓ Saved 5542 rows to: players_230021.csv\n",
      "\n",
      "[7/12] Scraping: Feb 22, 2023 (code: 230015)\n",
      "------------------------------------------------------------\n",
      "  → 600 players scraped...\n",
      "  → 1200 players scraped...\n",
      "  → 1800 players scraped...\n",
      "  → 2400 players scraped...\n",
      "  → 3000 players scraped...\n",
      "  → 3600 players scraped...\n",
      "  → 4200 players scraped...\n",
      "  → 4800 players scraped...\n",
      "  → 5400 players scraped...\n",
      "  ✓ Scraping complete at offset 5520\n",
      "  ✓ Total players collected: 5518\n",
      "  ✓ Saved 5518 rows to: players_230015.csv\n",
      "\n",
      "[8/12] Scraping: Jan 18, 2023 (code: 230010)\n",
      "------------------------------------------------------------\n",
      "  → 600 players scraped...\n",
      "  → 1200 players scraped...\n",
      "  → 1800 players scraped...\n",
      "  → 2400 players scraped...\n",
      "  → 3000 players scraped...\n",
      "  → 3600 players scraped...\n",
      "  → 4200 players scraped...\n",
      "  → 4800 players scraped...\n",
      "  → 5400 players scraped...\n",
      "  ✓ Scraping complete at offset 5580\n",
      "  ✓ Total players collected: 5539\n",
      "  ✓ Saved 5539 rows to: players_230010.csv\n",
      "\n",
      "[9/12] Scraping: Dec 17, 2022 (code: 230008)\n",
      "------------------------------------------------------------\n",
      "  → 600 players scraped...\n",
      "  → 1200 players scraped...\n",
      "  → 1800 players scraped...\n",
      "  → 2400 players scraped...\n",
      "  → 3000 players scraped...\n",
      "  → 3600 players scraped...\n",
      "  → 4200 players scraped...\n",
      "  → 4800 players scraped...\n",
      "  → 5400 players scraped...\n",
      "  ✓ Scraping complete at offset 5580\n",
      "  ✓ Total players collected: 5536\n",
      "  ✓ Saved 5536 rows to: players_230008.csv\n",
      "\n",
      "[10/12] Scraping: Nov 16, 2022 (code: 230006)\n",
      "------------------------------------------------------------\n",
      "  → 600 players scraped...\n",
      "  → 1200 players scraped...\n",
      "  → 1800 players scraped...\n",
      "  → 2400 players scraped...\n",
      "  → 3000 players scraped...\n",
      "  → 3600 players scraped...\n",
      "  → 4200 players scraped...\n",
      "  → 4800 players scraped...\n",
      "  → 5400 players scraped...\n",
      "  ✓ Scraping complete at offset 5520\n",
      "  ✓ Total players collected: 5507\n",
      "  ✓ Saved 5507 rows to: players_230006.csv\n",
      "\n",
      "[11/12] Scraping: Oct 7, 2022 (code: 230004)\n",
      "------------------------------------------------------------\n",
      "  → 600 players scraped...\n",
      "  → 1200 players scraped...\n",
      "  → 1800 players scraped...\n",
      "  → 2400 players scraped...\n",
      "  → 3000 players scraped...\n",
      "  → 3600 players scraped...\n",
      "  → 4200 players scraped...\n",
      "  → 4800 players scraped...\n",
      "  → 5400 players scraped...\n",
      "  ✓ Scraping complete at offset 5520\n",
      "  ✓ Total players collected: 5484\n",
      "  ✓ Saved 5484 rows to: players_230004.csv\n",
      "\n",
      "[12/12] Scraping: Sep 1, 2022 (code: 230001)\n",
      "------------------------------------------------------------\n",
      "  → 600 players scraped...\n",
      "  → 1200 players scraped...\n",
      "  → 1800 players scraped...\n",
      "  → 2400 players scraped...\n",
      "  → 3000 players scraped...\n",
      "  → 3600 players scraped...\n",
      "  → 4200 players scraped...\n",
      "  → 4800 players scraped...\n",
      "  → 5400 players scraped...\n",
      "  ✓ Scraping complete at offset 5520\n",
      "  ✓ Total players collected: 5481\n",
      "  ✓ Saved 5481 rows to: players_230001.csv\n",
      "Chrome WebDriver closed\n",
      "\n",
      "============================================================\n",
      "✓ SCRAPING COMPLETE\n",
      "  • Total roster dates: 12\n",
      "  • Total players collected: 66342\n",
      "  • Total files created: 12\n",
      "  • Saved to: /Users/lionlucky7/01.Projects/In-progress/soccer_prediction/2026_world_cup/data/player_data_train\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the scraper (saves each roster date to a separate CSV file)\n",
    "scraper.scrape_all_separate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bc6c78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
